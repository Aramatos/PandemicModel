{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "528c5c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import Env\n",
    "import numpy as np\n",
    "from gym.spaces import MultiDiscrete,Box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee7c4da0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "randint(low, high=None, size=None, dtype=int)\n",
       "\n",
       "Return random integers from `low` (inclusive) to `high` (exclusive).\n",
       "\n",
       "Return random integers from the \"discrete uniform\" distribution of\n",
       "the specified dtype in the \"half-open\" interval [`low`, `high`). If\n",
       "`high` is None (the default), then results are from [0, `low`).\n",
       "\n",
       ".. note::\n",
       "    New code should use the ``integers`` method of a ``default_rng()``\n",
       "    instance instead; please see the :ref:`random-quick-start`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "low : int or array-like of ints\n",
       "    Lowest (signed) integers to be drawn from the distribution (unless\n",
       "    ``high=None``, in which case this parameter is one above the\n",
       "    *highest* such integer).\n",
       "high : int or array-like of ints, optional\n",
       "    If provided, one above the largest (signed) integer to be drawn\n",
       "    from the distribution (see above for behavior if ``high=None``).\n",
       "    If array-like, must contain integer values\n",
       "size : int or tuple of ints, optional\n",
       "    Output shape.  If the given shape is, e.g., ``(m, n, k)``, then\n",
       "    ``m * n * k`` samples are drawn.  Default is None, in which case a\n",
       "    single value is returned.\n",
       "dtype : dtype, optional\n",
       "    Desired dtype of the result. Byteorder must be native.\n",
       "    The default value is int.\n",
       "\n",
       "    .. versionadded:: 1.11.0\n",
       "\n",
       "Returns\n",
       "-------\n",
       "out : int or ndarray of ints\n",
       "    `size`-shaped array of random integers from the appropriate\n",
       "    distribution, or a single such random int if `size` not provided.\n",
       "\n",
       "See Also\n",
       "--------\n",
       "random_integers : similar to `randint`, only for the closed\n",
       "    interval [`low`, `high`], and 1 is the lowest value if `high` is\n",
       "    omitted.\n",
       "Generator.integers: which should be used for new code.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> np.random.randint(2, size=10)\n",
       "array([1, 0, 0, 0, 1, 1, 0, 0, 1, 0]) # random\n",
       ">>> np.random.randint(1, size=10)\n",
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
       "\n",
       "Generate a 2 x 4 array of ints between 0 and 4, inclusive:\n",
       "\n",
       ">>> np.random.randint(5, size=(2, 4))\n",
       "array([[4, 0, 2, 1], # random\n",
       "       [3, 2, 2, 0]])\n",
       "\n",
       "Generate a 1 x 3 array with 3 different upper bounds\n",
       "\n",
       ">>> np.random.randint(1, [3, 5, 10])\n",
       "array([2, 2, 9]) # random\n",
       "\n",
       "Generate a 1 by 3 array with 3 different lower bounds\n",
       "\n",
       ">>> np.random.randint([1, 5, 7], 10)\n",
       "array([9, 8, 7]) # random\n",
       "\n",
       "Generate a 2 by 4 array using broadcasting with dtype of uint8\n",
       "\n",
       ">>> np.random.randint([1, 3, 5, 7], [[10], [20]], dtype=np.uint8)\n",
       "array([[ 8,  6,  9,  7], # random\n",
       "       [ 1, 16,  9, 12]], dtype=uint8)\n",
       "\u001b[0;31mType:\u001b[0m      builtin_function_or_method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.randint??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8870702d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulatepandemic(observation,actions):\n",
    "    #update observation\n",
    "    return np.random.randint(0,5,size=[4,5])\n",
    "\n",
    "def initializepandemic():\n",
    "    #observation could be vector of all individuals with states e.g. [0,1,2,0,3,0,...]\n",
    "    n = 100\n",
    "    return np.random.randint(0,5,size=[4,5])\n",
    "\n",
    "#write observation as n x states [[0,1,0,0,0],[1,0,0,0,0],...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e81cd525",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PanEnv(Env):\n",
    "    def __init__(self,n):#n number of people\n",
    "        self.n = n\n",
    "        self.observation = None\n",
    "        self.action_space = MultiDiscrete(nvec=[10,10,10,10])\n",
    "        '''multidiscrete mapping, NN will give us vector with values 0-10\n",
    "        we need to convert this map into meaning for our action > e.g. nvec[0]/sum(nvec) describes relative \n",
    "        availability. length of nvec == number of age groups'''\n",
    "        self.observation_space = Box(low=-np.inf,high=np.inf,shape=[4,5])\n",
    "    def step(self,actions):\n",
    "        observation = simulatepandemic(self.observation,actions)\n",
    "        #observation (object): agent's observation of the current environment\n",
    "        reward = np.sum(observation)\n",
    "        #reward (float) : amount of reward returned after previous action\n",
    "        #negative reward: punishment > change weights a lot, push away from causing weights, positive rewards pull\n",
    "        #do reward compared to reward from previous step\n",
    "        #naive example: reward = -sum(infected) > we want a reward where the cumulative sum of infections until end\n",
    "        #is minimized\n",
    "        #exp. solution: store information in self, summed infections, normalized by time\n",
    "        done = np.all(observation>1)#pairwise AND\n",
    "        #done (bool): whether the episode has ended, in which case further step() calls will return undefined results\n",
    "        info = {}\n",
    "        #info (dict): contains auxiliary diagnostic information (helpful for debugging, and sometimes learning)'''\n",
    "        return observation, reward, done, info\n",
    "    def reset(self):\n",
    "        #returns initial state\n",
    "        self.observation = initializepandemic()\n",
    "        return self.observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca0a4c5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4, 3, 1, 3, 1],\n",
       "       [4, 3, 2, 3, 2],\n",
       "       [1, 4, 2, 2, 4],\n",
       "       [1, 0, 4, 1, 1]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = PanEnv(n=100)\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d90faab",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = np.random.randint(1,5,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83f1932e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[2, 0, 0, 2, 0],\n",
       "        [1, 1, 3, 0, 4],\n",
       "        [4, 2, 4, 1, 2],\n",
       "        [0, 2, 4, 2, 0]]),\n",
       " 34,\n",
       " False,\n",
       " {})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bb93d1c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DummyVecEnv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8085/3931614852.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Parallel environments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDummyVecEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPanEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPPO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"MlpPolicy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#multilayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DummyVecEnv' is not defined"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "# Parallel environments\n",
    "\n",
    "env = DummyVecEnv([lambda: PanEnv(n=100)])\n",
    "\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1) #multilayer\n",
    "model.learn(total_timesteps=25000) #training loop\n",
    "#model.save(\"ppo_cartpole\")\n",
    "\n",
    "#del model # remove to demonstrate saving and loading\n",
    "\n",
    "#model = PPO.load(\"ppo_cartpole\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0924720e",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "while not done:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e733298c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into learning and testing\n",
    "model.learn(total_timesteps = 5000)\n",
    "# store/accumulate rewards"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
