{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "528c5c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import Env\n",
    "import numpy as np\n",
    "from gym.spaces import MultiDiscrete,Box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee7c4da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.randint??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8870702d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulatepandemic(observation,actions):\n",
    "    #update observation\n",
    "    return np.random.randint(0,5,size=[4,5])\n",
    "\n",
    "def initializepandemic():\n",
    "    #observation could be vector of all individuals with states e.g. [0,1,2,0,3,0,...]\n",
    "    n = 100\n",
    "    return np.random.randint(0,5,size=[4,5])\n",
    "\n",
    "#write observation as n x states [[0,1,0,0,0],[1,0,0,0,0],...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e81cd525",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PanEnv(Env):\n",
    "    def __init__(self,n):#n number of people\n",
    "        self.n = n\n",
    "        self.observation = None\n",
    "        self.action_space = MultiDiscrete(nvec=[10,10,10,10])\n",
    "        '''multidiscrete mapping, NN will give us vector with values 0-10\n",
    "        we need to convert this map into meaning for our action > e.g. nvec[0]/sum(nvec) describes relative \n",
    "        availability. length of nvec == number of age groups'''\n",
    "        self.observation_space = Box(low=-np.inf,high=np.inf,shape=[4,5])\n",
    "    def step(self,actions):\n",
    "        observation = simulatepandemic(self.observation,actions)\n",
    "        #observation (object): agent's observation of the current environment\n",
    "        reward = np.sum(observation)\n",
    "        #reward (float) : amount of reward returned after previous action\n",
    "        #negative reward: punishment > change weights a lot, push away from causing weights, positive rewards pull\n",
    "        #do reward compared to reward from previous step\n",
    "        #naive example: reward = -sum(infected) > we want a reward where the cumulative sum of infections until end\n",
    "        #is minimized\n",
    "        #exp. solution: store information in self, summed infections, normalized by time\n",
    "        done = np.all(observation>1)#pairwise AND\n",
    "        #done (bool): whether the episode has ended, in which case further step() calls will return undefined results\n",
    "        info = {}\n",
    "        #info (dict): contains auxiliary diagnostic information (helpful for debugging, and sometimes learning)'''\n",
    "        return observation, reward, done, info\n",
    "    def reset(self):\n",
    "        #returns initial state\n",
    "        self.observation = initializepandemic()\n",
    "        return self.observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ca0a4c5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 3, 0, 3, 3],\n",
       "       [1, 0, 0, 0, 4],\n",
       "       [2, 0, 4, 4, 0],\n",
       "       [3, 2, 3, 1, 2]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = PanEnv(n=100)\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9d90faab",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = np.random.randint(1,5,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "83f1932e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[3, 0, 2, 2, 3],\n",
       "        [2, 1, 1, 2, 4],\n",
       "        [4, 2, 4, 1, 2],\n",
       "        [3, 1, 2, 4, 4]]),\n",
       " 47,\n",
       " False,\n",
       " {})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0bb93d1c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'stable_baselines3'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9589/3931614852.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mstable_baselines3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPPO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstable_baselines3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_util\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmake_vec_env\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Parallel environments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'stable_baselines3'"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "# Parallel environments\n",
    "\n",
    "env = DummyVecEnv([lambda: PanEnv(n=100)])\n",
    "\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1) #multilayer\n",
    "model.learn(total_timesteps=25000) #training loop\n",
    "#model.save(\"ppo_cartpole\")\n",
    "\n",
    "#del model # remove to demonstrate saving and loading\n",
    "\n",
    "#model = PPO.load(\"ppo_cartpole\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0924720e",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "while not done:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e733298c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into learning and testing\n",
    "model.learn(total_timesteps = 5000)\n",
    "# store/accumulate rewards"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
